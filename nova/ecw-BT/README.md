# ECW-BT Level-0: SBERT-Seeded Word Embeddings

Production-grade word vector training pipeline that:
- Seeds from SBERT geometry (teacher, immutable)
- Learns Wikipedia co-occurrence via CCD physics
- Prevents collapse and catastrophic forgetting via fusion anchoring
- Optimized for Apple Silicon (MPS) with pre-generated negatives

> **What this is NOT**: This is not a neural language model or a transformer replacement. ECW-BT produces word-level geometry that higher-level systems (Nova/Livnium) compose and reason over.

## Design Philosophy

- **Geometry first, labels second**: Word vectors encode spatial relationships, not task-specific signals
- **Teacher provides structure, student provides adaptation**: SBERT seed anchors semantics; CCD physics adapts to co-occurrence
- **Stability > peak benchmark score**: Fusion anchoring prevents catastrophic forgetting; acceptance criteria ensure reproducibility

## Architecture

```mermaid
graph LR
    A[Wikipedia JSONL] -->|build_vocab.py| B[vocab.txt<br/>freq.npy]
    A -->|build_pairs.py| C[pairs_pos_*.bin]
    B -->|make_sbert_seed.py| D[V_seed.npy<br/>Teacher]
    C -->|distill_pairwise.py| E[Training Loop]
    D -->|distill_pairwise.py| E
    E -->|Fusion Anchoring| F[ecw_bt_vectors.npy]
    F -->|validate_level0.py| G[Acceptance Report]
    
    style D fill:#e1f5ff
    style F fill:#fff4e1
    style G fill:#e8f5e9
```

**Pipeline Flow:**
1. **Phase 0**: Create SBERT seed (`V_seed.npy`) - teacher geometry, immutable
2. **Phase 1**: Build vocab and co-occurrence pairs from Wikipedia
3. **Phase 2**: Train student vectors with CCD physics + fusion anchoring
4. **Phase 3**: Validate acceptance criteria (drift, collapse, neighbors)

## Quick Start

### Full Pipeline (First Time)

```bash
cd nova/ecw-BT

# 1. Clean start (optional but recommended)
./clean_start.sh

# 2. Build vocab and frequency table
python scripts/build_vocab.py \
  --wiki-paths wikipedia/wiki_extractor_src/extracted/AA/wiki_00 \
  --out-dir data \
  --max-vocab 50000 \
  --write-mass-table

# 3. Build positive pair shards
python scripts/build_pairs.py \
  --wiki-paths wikipedia/wiki_extractor_src/extracted/AA/wiki_00 \
  --vocab data/vocab.txt \
  --out-dir data/pairs \
  --window 5 \
  --pairs-per-shard 5000000 \
  --symmetric

# 4. Create SBERT seed (teacher vectors)
python scripts/make_sbert_seed.py \
  --vocab data/vocab.txt \
  --output data/V_seed.npy \
  --dim 256 \
  --model sentence-transformers/all-mpnet-base-v2

# 5. Train with CCD + fusion
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
export PYTORCH_ENABLE_MPS_FALLBACK=1

python scripts/distill_pairwise.py \
  --seed data/V_seed.npy \
  --pairs data/pairs/pairs_pos_*.bin \
  --vocab data/vocab.txt \
  --freq data/freq.npy \
  --output data/ecw_bt_vectors.npy \
  --dim 256 \
  --lr 1e-3 \
  --negatives 5 \
  --epochs 3 \
  --device mps \
  --pairs-per-step 1000000

# 6. Validate acceptance
python scripts/validate_level0.py \
  --checkpoint data/ecw_bt_vectors.npy \
  --seed data/V_seed.npy \
  --mass-table data/mass_table.json \
  --vocab data/vocab.txt
```

### Quick Test (Small Vocab)

```bash
# Smaller vocab for faster testing
python scripts/build_vocab.py --wiki-paths wikipedia/... --out-dir data --max-vocab 10000 --write-mass-table
python scripts/build_pairs.py --wiki-paths wikipedia/... --vocab data/vocab.txt --out-dir data/pairs --window 5
python scripts/make_sbert_seed.py --vocab data/vocab.txt --output data/V_seed.npy --dim 256
python scripts/distill_pairwise.py --seed data/V_seed.npy --pairs data/pairs/pairs_pos_*.bin --vocab data/vocab.txt --freq data/freq.npy --output data/ecw_bt_vectors.npy --dim 256 --epochs 1 --pairs-per-step 500000
```

## Scripts Overview

| Script | Purpose | Phase | Input | Output |
|--------|---------|-------|-------|--------|
| `scripts/build_vocab.py` | Build vocabulary | 1A | Wikipedia JSONL | `vocab.txt`, `freq.npy` |
| `scripts/build_pairs.py` | Extract co-occurrence pairs | 1B | `vocab.txt`, Wikipedia | `pairs_pos_*.bin` |
| `scripts/make_sbert_seed.py` | Create teacher vectors | 0 | `vocab.txt` | `V_seed.npy` |
| `scripts/distill_pairwise.py` | Train with CCD + fusion | 2 | pairs, seed | `ecw_bt_vectors.npy` |
| `scripts/validate_level0.py` | Acceptance testing | 3 | vectors, seed | `validate_level0.log` |
| `tools/probe_galaxy.py` | Interactive probing | - | checkpoint | neighbors, sentences |
| `tools/analogy_test.py` | Analogy evaluation | - | checkpoint | accuracy report |
| `tools/visualize_galaxy.py` | UMAP visualization | - | checkpoint | `galaxy_map_hd.html` |
| `clean_start.sh` | Clean cache/prep | - | - | cleaned workspace |

## Core Concepts

### SBERT Seed (Teacher)
- Immutable geometry from `sentence-transformers/all-mpnet-base-v2`
- Projected to target dimension (default: 256)
- Provides semantic structure that student learns to deform

### CCD Physics
- **Divergence pivot**: `align_barrier = 0.38` (cosine similarity threshold)
- **Positives**: Pull together (attraction toward barrier)
- **Negatives**: Push apart (repulsion when above barrier)
- **Force shaping**: Bounded via `tanh` or `clip` to prevent explosions

### Fusion Anchoring
- Prevents catastrophic forgetting: `V_student = Î»Â·V_student + (1-Î»)Â·V_seed`
- Lambda schedule: epoch1=0.05, epoch2=0.10, epoch3+=0.15
- Teacher acts as elastic scaffold

### Pre-generated Negatives
- Negatives sampled once per shard (not per batch)
- Eliminates CPUâ†’GPU transfers in hot loop
- Chunked processing (200k pairs) to avoid OOM

## Performance

### Expected Throughput
- **~100-150k pairs/sec** on Apple Silicon (MPS)
- **Memory**: ~2-3GB for 50k vocab Ã— 256 dim
- **Pre-generation**: ~1-2s per shard (one-time cost)
- **Training**: ~8-10s per 1M pair batch

### Known Limitations
- **MPS OOM**: With >1M pairs Ã— 5 negatives, chunking handles this automatically
- **Auto-optimize**: Disabled by default (reducing batch size hurts performance)
- **Large vocab**: >200k vocab may need `--pairs-per-step` reduction
- **CPU fallback**: Use `--device cpu` if MPS crashes persist

### Optimization Tips
- Use `--pairs-per-step 1000000` or higher (fewer scatters = faster)
- Pre-generation overhead is one-time per shard (worth it)
- Disable `--auto-optimize` (it reduces batch size and hurts performance)
- For testing: use `--negatives 3` and smaller vocab

## Probing and Evaluation

### Interactive Probing
```bash
python tools/probe_galaxy.py \
  --checkpoint data/ecw_bt_vectors.npy \
  --mass data/mass_table.json \
  --query kitten \
  --topk 15

python tools/probe_galaxy.py \
  --checkpoint data/ecw_bt_vectors.npy \
  --mass data/mass_table.json \
  --sentence "a small cat chased a mouse" \
  --topk 15 \
  --resonance 0.5
```

### Analogy Testing
```bash
python tools/analogy_test.py \
  --checkpoint data/ecw_bt_vectors.npy \
  --mass-table data/mass_table.json
```

### Visualization
```bash
python tools/visualize_galaxy.py \
  --checkpoint data/ecw_bt_vectors.npy \
  --mass data/mass_table.json \
  --limit 10000
# Opens tools/galaxy_map_hd.html in browser
```

## Legacy Scripts

These scripts are kept for reference but are not part of the main Level-0 pipeline (located in `legacy/`):

- **`legacy/train_ecw_bt.py`**: Old window-based trainer (streaming windows, no SBERT seed)
- **`legacy/inject_sbert_pairwise.py`**: Post-processing SBERT injection (refines existing vectors)
- **`legacy/distill_rotate.py`**: Procrustes rotation alignment (whitening + rotation to SBERT space)
- **`legacy/apply_rotated_vectors.py`**: Apply rotation transforms

## File Structure

```
nova/ecw-BT/
â”œâ”€â”€ scripts/                   # Level-0 pipeline scripts
â”‚   â”œâ”€â”€ build_vocab.py         # Phase 1A: Build vocab
â”‚   â”œâ”€â”€ build_pairs.py          # Phase 1B: Build pairs
â”‚   â”œâ”€â”€ make_sbert_seed.py     # Phase 0: Create seed
â”‚   â”œâ”€â”€ distill_pairwise.py    # Phase 2: Main trainer
â”‚   â””â”€â”€ validate_level0.py     # Phase 3: Validation
â”œâ”€â”€ tools/                      # Utility scripts
â”‚   â”œâ”€â”€ probe_galaxy.py        # Interactive probing
â”‚   â”œâ”€â”€ analogy_test.py        # Analogy evaluation
â”‚   â”œâ”€â”€ visualize_galaxy.py    # UMAP visualization
â”‚   â””â”€â”€ galaxy_map_hd.html     # Visualization output
â”œâ”€â”€ legacy/                     # Legacy/experimental scripts
â”‚   â”œâ”€â”€ train_ecw_bt.py        # Old window-based trainer
â”‚   â”œâ”€â”€ inject_sbert_pairwise.py
â”‚   â”œâ”€â”€ distill_rotate.py
â”‚   â””â”€â”€ apply_rotated_vectors.py
â”œâ”€â”€ src/                        # Core modules
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ physics.py             # Force laws
â”‚   â”œâ”€â”€ data_loader.py          # Data streaming
â”‚   â”œâ”€â”€ trainer.py             # Legacy trainer
â”‚   â””â”€â”€ collapse_engine.py     # Gravity pooling
â”œâ”€â”€ data/                       # Data artifacts
â”‚   â”œâ”€â”€ vocab.txt              # Vocabulary (id â†’ token)
â”‚   â”œâ”€â”€ freq.npy               # Token frequencies
â”‚   â”œâ”€â”€ mass_table.json        # Legacy format (word â†’ freq/mass)
â”‚   â”œâ”€â”€ V_seed.npy             # SBERT seed vectors (teacher)
â”‚   â”œâ”€â”€ ecw_bt_vectors.npy     # Final trained vectors
â”‚   â””â”€â”€ pairs/                 # Pair shards
â”‚       â”œâ”€â”€ pairs_pos_*.bin    # Pair shards (int32 binary)
â”‚       â””â”€â”€ pairs_meta.json    # Pair metadata
â”œâ”€â”€ checkpoints/                # Training checkpoints
â”œâ”€â”€ logs/                       # Training logs
â”œâ”€â”€ wikipedia/                  # Raw Wikipedia data
â”œâ”€â”€ clean_start.sh              # Clean cache script
â”œâ”€â”€ requirements-ecw-bt.txt     # Python dependencies
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ PLAN.md                     # Original plan (historical)
```

## Acceptance Criteria

After training, vectors should pass `validate_level0.py`:

- **Teacher drift**: Mean cosine similarity >0.5 (retains SBERT structure)
- **No collapse**: Random pair cosine <0.8 (vectors don't all point same direction)
- **Sane neighbors**: Nearest neighbors are semantically related
- **Analogy sanity**: Common analogies improve over random and SBERT baseline (not expected to match Word2Vec benchmarks)

> ðŸš« **FREEZE RULE**: Once `validate_level0.py` passes, **DO NOT retrain or fine-tune these vectors**. Level-0 becomes immutable infrastructure for higher-level systems. Treat it as a frozen artifact, not a tunable model.

## Troubleshooting

### MPS Crashes
```bash
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
export PYTORCH_ENABLE_MPS_FALLBACK=1
# Or use --device cpu
```

### Slow Training
- Increase `--pairs-per-step` (default: 1M)
- Reduce `--negatives` (default: 5, try 3)
- Disable `--auto-optimize` (it's disabled by default)
- Check system load: `--cpu-threads 2 --throttle 0.01` if needed

### OOM Errors
- Reduce `--pairs-per-step` to 500000
- Reduce `--negatives` to 3
- Use `--device cpu` (slower but more stable)

### Validation Fails
- Check teacher drift (should be gradual, not cliff)
- Increase fusion lambda if drift too high
- Check for collapse (all vectors pointing same direction)
- Verify negatives are working (increase `--negatives`)

## References

- **CCD Physics**: Divergence-based force law with barrier at 0.38
- **Fusion Anchoring**: Elastic scaffold prevents catastrophic forgetting
- **SBERT**: `sentence-transformers/all-mpnet-base-v2` (768-dim â†’ projected to target)
- **Word2Vec-style negatives**: Frequency-based sampling (freq^0.75)
